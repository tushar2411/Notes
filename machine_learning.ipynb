{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9LgvnrBsUkluwZRIn/FEF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushar2411/Notes/blob/main/machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is meant by machine learning?\n",
        "\n",
        "Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy."
      ],
      "metadata": {
        "id": "CaJqyVAjdnqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean, Median, Mode:\n",
        "\n",
        "Mean: The arithmetic mean is the sum of all data points divided by the number of data points."
      ],
      "metadata": {
        "id": "btagYbq3d7Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5])\n",
        "mean = np.mean(data)\n",
        "print(mean)\n"
      ],
      "metadata": {
        "id": "5Km2ZT6Ud_Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: 3.0\n",
        "\n",
        "Median: The median is the middle value of a dataset when it is ordered from smallest to largest."
      ],
      "metadata": {
        "id": "lHwPl3UUeEdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5])\n",
        "median = np.median(data)\n",
        "print(median)\n"
      ],
      "metadata": {
        "id": "O7dDiOKceCpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: 3.0\n",
        "\n",
        "Mode: The mode is the value that appears most frequently in a dataset."
      ],
      "metadata": {
        "id": "BTrSEkOVeLSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "data = [1, 2, 3, 3, 4, 5]\n",
        "mode = statistics.mode(data)\n",
        "print(mode)\n"
      ],
      "metadata": {
        "id": "1RaxgVXIeO4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: 1.41421356\n",
        "\n",
        "Percentile: The percentile is the value below which a given percentage of observations in a dataset falls."
      ],
      "metadata": {
        "id": "rwaKnA0TeQi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5])\n",
        "percentile_75 = np.percentile(data, 75)\n",
        "print(percentile_75)\n"
      ],
      "metadata": {
        "id": "wPk-DiyVeS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: 4.0\n",
        "\n",
        "Data Distribution: Data distribution describes how the values in a dataset are spread out or distributed."
      ],
      "metadata": {
        "id": "3rMsZr6eeOQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = np.random.normal(0, 1, 1000) # generate 1000 random numbers from a normal distribution\n",
        "plt.hist(data, bins=50) # plot a histogram with 50 bins\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a6Gn6ELHeZS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A histogram plot showing the distribution of random numbers generated from a normal distribution.\n",
        "\n",
        "Normal Data Distribution: A normal data distribution is a type of data distribution where the values in a dataset are symmetrically distributed around the mean."
      ],
      "metadata": {
        "id": "jhgXidKoebnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter Plot: \n",
        "A scatter plot is a type of plot used to display the relationship between two variables."
      ],
      "metadata": {
        "id": "1I99_jP6ekDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.normal(0, 1, 100) # generate 100 random numbers from a normal distribution\n",
        "y = 2 * x + np.random.normal(0, 1, 100) # create a second variable that is linearly related to the first variable, with some added noise\n",
        "plt.scatter(x, y) #\n"
      ],
      "metadata": {
        "id": "KwRBy9hReoWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression: \n",
        "\n",
        "Linear regression is a technique used to model the relationship between a dependent variable and one or more independent variables."
      ],
      "metadata": {
        "id": "WYgTv-fKeqsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "x = np.random.normal(0, 1, 100) # generate 100 random numbers from a normal distribution\n",
        "y = 2 * x + np.random.normal(0, 1, 100) # create a second variable that is linearly related to the first variable, with some added noise\n",
        "model = LinearRegression().fit(x.reshape(-1,1), y) # fit a linear regression model to the data\n",
        "y_pred = model.predict(x.reshape(-1,1)) # use the model to make predictions\n",
        "plt.scatter(x, y) # plot the original data\n",
        "plt.plot(x, y_pred, color='red') # plot the predicted values\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C9buK7ZCesf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A scatter plot of the original data and a line representing the predicted values from a linear regression model.\n",
        "\n",
        "Polynomial Regression: \n",
        "\n",
        "\n",
        "Polynomial regression is a technique used to model the relationship between a dependent variable and one or more independent variables using a polynomial function."
      ],
      "metadata": {
        "id": "KJSzg6Adevhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "x = np.random.normal(0, 1, 100) # generate 100 random numbers from a normal distribution\n",
        "y = 2 * x + x**2 + np.random.normal(0, 1, 100) # create a second variable that is related to the first variable by a quadratic function, with some added noise\n",
        "poly = PolynomialFeatures(degree=2) # create a polynomial feature transformer with degree 2\n",
        "x_poly = poly.fit_transform(x.reshape(-1,1)) # transform the original data into a polynomial feature set\n",
        "model = LinearRegression().fit(x_poly, y) # fit a linear regression model to the polynomial feature set\n",
        "y_pred = model.predict(x_poly) # use the model to make predictions\n",
        "plt.scatter(x, y) # plot the original data\n",
        "plt.plot(x, y_pred, color='red') # plot the predicted values\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_3LxOWpceyW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A scatter plot of the original data and a curve representing the predicted values from a polynomial regression model.\n",
        "\n",
        "Multiple Regression: \n",
        "\n",
        "\n",
        "Multiple regression is a technique used to model the relationship between a dependent variable and two or more independent variables."
      ],
      "metadata": {
        "id": "wXh8KyuNe1Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "x1 = np.random.normal(0, 1, 100) # generate 100 random numbers from a normal distribution for the first independent variable\n",
        "x2 = np.random.normal(0, 1, 100) # generate 100 random numbers from a normal distribution for the second independent variable\n",
        "y = 2 * x1 + 3 * x2 + np.random.normal(0, 1, 100) # create a dependent variable that is linearly related to the independent variables, with some added noise\n",
        "X = np.column_stack((x1, x2)) # stack the independent variables horizontally to create a feature set\n",
        "model = LinearRegression().fit(X, y) # fit a linear regression model to the data\n",
        "y_pred = model.predict(X) # use the model to make predictions\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(x1, x2, y) # plot the original data in 3D\n",
        "ax.plot_trisurf(x1, x2, y_pred, color='red', alpha=0.5) # plot the predicted values as a surface in 3\n"
      ],
      "metadata": {
        "id": "kPf1cuBte4yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale:\n",
        "\n",
        " Scale refers to the range of values of a variable, and it can be measured using various methods such as nominal, ordinal, interval, and ratio scales.\n",
        "\n",
        "Train/Test: \n",
        "\n",
        "Train/test is a technique used to evaluate the performance of a machine learning model. The data is split into a training set and a testing set, and the model is trained on the training set and evaluated on the testing set to see how well it generalizes to new data.\n",
        "\n",
        "Decision Tree:\n",
        "\n",
        " A decision tree is a hierarchical model that partitions the data into subsets based on the values of the independent variables, and it is often used for classification and regression tasks."
      ],
      "metadata": {
        "id": "ammzuyvzfC6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "plot_tree(clf)\n"
      ],
      "metadata": {
        "id": "_4KozvUNfGh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A visualization of the decision tree model that was fitted to the iris dataset.\n",
        "\n",
        "Confusion Matrix: \n",
        "\n",
        "A confusion matrix is a table that shows the true positives, true negatives, false positives, and false negatives of a classification model, and it is often used to evaluate the performance of the model."
      ],
      "metadata": {
        "id": "Gjsmw-6QfIpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=3, n_classes=2, random_state=42)\n",
        "y_pred = [0 if x[0] < 0 else 1 for x in X] # make random predictions\n",
        "cm = confusion_matrix(y, y_pred) # calculate the confusion matrix\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "CpkvoLL_fMPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A scatter plot of the random data points, with different colors indicating the two clusters that were identified by the hierarchical clustering model.\n",
        "\n",
        "Logistic Regression: \n",
        "\n",
        "\n",
        "Logistic regression is a technique used to model the probability of a binary outcome (e.g., 0 or 1) based on one or more independent variables, and it is often used for classification tasks."
      ],
      "metadata": {
        "id": "tIUPTrczfLNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x = np.random.normal(0, 1, 100) # generate 100 random numbers from a normal distribution\n",
        "y = np.where(x > 0, 1, 0) # create a binary outcome variable based on the values of the independent variable\n",
        "model = LogisticRegression().fit(x.reshape(-1,1), y) # fit a logistic regression model to the data\n",
        "y_pred = model.predict_proba(x.reshape(-1,1))[:,1] # use the model to predict the probability of the positive outcome\n",
        "plt.scatter(x, y) # plot the original data\n",
        "plt.plot(x, y_pred, color='red') # plot the predicted probabilities\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7AZsQ15-fVfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search:\n",
        "\n",
        "\n",
        " Grid search is a technique used to find the optimal hyperparameters for a machine learning model by exhaustively searching through a specified set of hyperparameters and evaluating the performance of the model for each combination."
      ],
      "metadata": {
        "id": "ElKY8xCgfeM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "parameters = {'kernel': ('linear', 'rbf'), 'C': [1, 10]} # define the hyperparameter grid to search over\n",
        "clf = SVC()\n",
        "grid_search = GridSearchCV(clf, parameters)\n",
        "grid_search.fit(X, y) # perform the grid search\n",
        "print(grid_search.best_params_) # print the best hyperparameters that were found\n"
      ],
      "metadata": {
        "id": "73bsjVcRfg8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: The best hyperparameters that were found for the support vector machine (SVM) model when applied to the iris dataset.\n",
        "\n",
        "Categorical Data: \n",
        "\n",
        "\n",
        "Categorical data refers to data that takes on discrete values, such as gender (male/female) or education level (high school/college/graduate)."
      ],
      "metadata": {
        "id": "T2NxwPCLfjDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'gender': ['male', 'male', 'female', 'male', 'female'], 'education': ['college', 'high school', 'graduate', 'graduate', 'high school']}) # create a dataframe with two categorical variables\n",
        "df = pd.get_dummies(df) # use one-hot encoding to convert the categorical variables to binary variables\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "x6xzx967flq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A dataframe with two categorical variables that have been converted to binary variables using one-hot encoding.\n",
        "\n",
        "K-means:\n",
        "\n",
        "\n",
        " K-means is a clustering technique used to group similar data points together based on their distances from a specified number of centroids, and it is often used for exploratory data analysis and pattern recognition."
      ],
      "metadata": {
        "id": "xO3JUvPhfn60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X = np.random.normal(0, 1, (100, 2)) # generate 100 random 2D points from a normal distribution\n",
        "model = KMeans(n_clusters=2).fit(X) # fit a K-means clustering model to the data with 2 clusters\n",
        "plt.scatter(X[:,0], X[:,1], c=model.labels_) # plot the data with different colors for each cluster\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aXqY_p7Efr3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: A scatter plot of the random data points, with different colors indicating the two clusters that were identified by the K-means clustering model.\n",
        "\n",
        "Bootstrap Aggregation: \n",
        "\n",
        "\n",
        "Bootstrap aggregation (or bagging) is a technique used to improve the stability and accuracy of a machine learning model by training multiple models on different subsets of the data and combining their predictions."
      ],
      "metadata": {
        "id": "ULiIOcsKft7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=3, n_classes=2, random_state=42)\n",
        "clf = DecisionTreeClassifier()\n",
        "model = BaggingClassifier(base_estimator=clf, n_estimators=10)\n",
        "model.fit(X, y) # fit a bagged decision tree model to the data\n",
        "print(model.predict(X[:10])) # print the predicted classes for the first 10 data points\n"
      ],
      "metadata": {
        "id": "oxFaVZgQfwdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: The predicted classes for the first 10 data points using the bagged decision tree model.\n",
        "\n",
        "Cross Validation:\n",
        "\n",
        "\n",
        " Cross validation is a technique used to estimate the performance of a machine learning model by repeatedly splitting the data into training and testing sets, and averaging the results to get a more accurate estimate of the model's performance."
      ],
      "metadata": {
        "id": "QNboBrqxfyQs"
      }
    }
  ]
}